{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1oRKWNkSM889CBKcpD3IU8iOFAC35eDBW","authorship_tag":"ABX9TyOHy/g5qI7iVG/wwFMpTwpc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"id":"TPTJNhD-OvZY","executionInfo":{"status":"ok","timestamp":1680820782517,"user_tz":-330,"elapsed":8,"user":{"displayName":"Ayush Agarwal .","userId":"08062159735606572785"}},"outputId":"ecea7d35-651b-493d-b97d-3d8151c71e13","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Hello world\n","Hello World!\n"]}],"source":["print(\"Hello world\")\n","\n","# String concatenation\n","first = \"Hello \"\n","second = \"World\"\n","long_result = first + second + \"!\"\n","print(long_result)"]},{"cell_type":"code","source":["import pandas as pd\n","from google.colab import data_table as dt\n","dt.enable_dataframe_formatter()\n","dt.disable_dataframe_formatter()\n","airline_passengers = pd.read_csv('/content/drive/MyDrive/airline-passengers.csv')\n","airline_passengers"],"metadata":{"id":"NPN5oqsnX5xQ","executionInfo":{"status":"ok","timestamp":1680820806382,"user_tz":-330,"elapsed":466,"user":{"displayName":"Ayush Agarwal .","userId":"08062159735606572785"}},"outputId":"662441a0-c028-4d2e-ca34-897d74990772","colab":{"base_uri":"https://localhost:8080/","height":423}},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       Month  Passengers\n","0    1949-01         112\n","1    1949-02         118\n","2    1949-03         132\n","3    1949-04         129\n","4    1949-05         121\n","..       ...         ...\n","139  1960-08         606\n","140  1960-09         508\n","141  1960-10         461\n","142  1960-11         390\n","143  1960-12         432\n","\n","[144 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-1fb21964-9300-4412-ad71-5fefa66e602e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Month</th>\n","      <th>Passengers</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1949-01</td>\n","      <td>112</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1949-02</td>\n","      <td>118</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1949-03</td>\n","      <td>132</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1949-04</td>\n","      <td>129</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1949-05</td>\n","      <td>121</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>139</th>\n","      <td>1960-08</td>\n","      <td>606</td>\n","    </tr>\n","    <tr>\n","      <th>140</th>\n","      <td>1960-09</td>\n","      <td>508</td>\n","    </tr>\n","    <tr>\n","      <th>141</th>\n","      <td>1960-10</td>\n","      <td>461</td>\n","    </tr>\n","    <tr>\n","      <th>142</th>\n","      <td>1960-11</td>\n","      <td>390</td>\n","    </tr>\n","    <tr>\n","      <th>143</th>\n","      <td>1960-12</td>\n","      <td>432</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>144 rows Ã— 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1fb21964-9300-4412-ad71-5fefa66e602e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1fb21964-9300-4412-ad71-5fefa66e602e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1fb21964-9300-4412-ad71-5fefa66e602e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["airline_passengers.shape\n","airline_passengers.describe()\n","airline_passengers.values"],"metadata":{"id":"K3Ipgb4pcg2R","executionInfo":{"status":"ok","timestamp":1680820809594,"user_tz":-330,"elapsed":4,"user":{"displayName":"Ayush Agarwal .","userId":"08062159735606572785"}},"outputId":"aa29931b-698e-44c2-f066-5a0867ad1bd3","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([['1949-01', 112],\n","       ['1949-02', 118],\n","       ['1949-03', 132],\n","       ['1949-04', 129],\n","       ['1949-05', 121],\n","       ['1949-06', 135],\n","       ['1949-07', 148],\n","       ['1949-08', 148],\n","       ['1949-09', 136],\n","       ['1949-10', 119],\n","       ['1949-11', 104],\n","       ['1949-12', 118],\n","       ['1950-01', 115],\n","       ['1950-02', 126],\n","       ['1950-03', 141],\n","       ['1950-04', 135],\n","       ['1950-05', 125],\n","       ['1950-06', 149],\n","       ['1950-07', 170],\n","       ['1950-08', 170],\n","       ['1950-09', 158],\n","       ['1950-10', 133],\n","       ['1950-11', 114],\n","       ['1950-12', 140],\n","       ['1951-01', 145],\n","       ['1951-02', 150],\n","       ['1951-03', 178],\n","       ['1951-04', 163],\n","       ['1951-05', 172],\n","       ['1951-06', 178],\n","       ['1951-07', 199],\n","       ['1951-08', 199],\n","       ['1951-09', 184],\n","       ['1951-10', 162],\n","       ['1951-11', 146],\n","       ['1951-12', 166],\n","       ['1952-01', 171],\n","       ['1952-02', 180],\n","       ['1952-03', 193],\n","       ['1952-04', 181],\n","       ['1952-05', 183],\n","       ['1952-06', 218],\n","       ['1952-07', 230],\n","       ['1952-08', 242],\n","       ['1952-09', 209],\n","       ['1952-10', 191],\n","       ['1952-11', 172],\n","       ['1952-12', 194],\n","       ['1953-01', 196],\n","       ['1953-02', 196],\n","       ['1953-03', 236],\n","       ['1953-04', 235],\n","       ['1953-05', 229],\n","       ['1953-06', 243],\n","       ['1953-07', 264],\n","       ['1953-08', 272],\n","       ['1953-09', 237],\n","       ['1953-10', 211],\n","       ['1953-11', 180],\n","       ['1953-12', 201],\n","       ['1954-01', 204],\n","       ['1954-02', 188],\n","       ['1954-03', 235],\n","       ['1954-04', 227],\n","       ['1954-05', 234],\n","       ['1954-06', 264],\n","       ['1954-07', 302],\n","       ['1954-08', 293],\n","       ['1954-09', 259],\n","       ['1954-10', 229],\n","       ['1954-11', 203],\n","       ['1954-12', 229],\n","       ['1955-01', 242],\n","       ['1955-02', 233],\n","       ['1955-03', 267],\n","       ['1955-04', 269],\n","       ['1955-05', 270],\n","       ['1955-06', 315],\n","       ['1955-07', 364],\n","       ['1955-08', 347],\n","       ['1955-09', 312],\n","       ['1955-10', 274],\n","       ['1955-11', 237],\n","       ['1955-12', 278],\n","       ['1956-01', 284],\n","       ['1956-02', 277],\n","       ['1956-03', 317],\n","       ['1956-04', 313],\n","       ['1956-05', 318],\n","       ['1956-06', 374],\n","       ['1956-07', 413],\n","       ['1956-08', 405],\n","       ['1956-09', 355],\n","       ['1956-10', 306],\n","       ['1956-11', 271],\n","       ['1956-12', 306],\n","       ['1957-01', 315],\n","       ['1957-02', 301],\n","       ['1957-03', 356],\n","       ['1957-04', 348],\n","       ['1957-05', 355],\n","       ['1957-06', 422],\n","       ['1957-07', 465],\n","       ['1957-08', 467],\n","       ['1957-09', 404],\n","       ['1957-10', 347],\n","       ['1957-11', 305],\n","       ['1957-12', 336],\n","       ['1958-01', 340],\n","       ['1958-02', 318],\n","       ['1958-03', 362],\n","       ['1958-04', 348],\n","       ['1958-05', 363],\n","       ['1958-06', 435],\n","       ['1958-07', 491],\n","       ['1958-08', 505],\n","       ['1958-09', 404],\n","       ['1958-10', 359],\n","       ['1958-11', 310],\n","       ['1958-12', 337],\n","       ['1959-01', 360],\n","       ['1959-02', 342],\n","       ['1959-03', 406],\n","       ['1959-04', 396],\n","       ['1959-05', 420],\n","       ['1959-06', 472],\n","       ['1959-07', 548],\n","       ['1959-08', 559],\n","       ['1959-09', 463],\n","       ['1959-10', 407],\n","       ['1959-11', 362],\n","       ['1959-12', 405],\n","       ['1960-01', 417],\n","       ['1960-02', 391],\n","       ['1960-03', 419],\n","       ['1960-04', 461],\n","       ['1960-05', 472],\n","       ['1960-06', 535],\n","       ['1960-07', 622],\n","       ['1960-08', 606],\n","       ['1960-09', 508],\n","       ['1960-10', 461],\n","       ['1960-11', 390],\n","       ['1960-12', 432]], dtype=object)"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["Data Preperation for univariate Analysis considering as a CNN model"],"metadata":{"id":"-_SgPuo906AK"}},{"cell_type":"code","source":["# split a univariate sequence into samples\n","def split_sequence(sequence, n_steps):\n","\tX, y = list(), list()\n","\tfor i in range(len(sequence)):\n","\t\t# find the end of this pattern\n","\t\tend_ix = i + n_steps\n","\t\t# check if we are beyond the sequence\n","\t\tif end_ix > len(sequence)-1:\n","\t\t\tbreak\n","\t\t# gather input and output parts of the pattern\n","\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n","\t\tX.append(seq_x)\n","\t\ty.append(seq_y)\n","\treturn array(X), array(y)"],"metadata":{"id":"skfPhWDO0z7Y","executionInfo":{"status":"ok","timestamp":1680820812269,"user_tz":-330,"elapsed":3,"user":{"displayName":"Ayush Agarwal .","userId":"08062159735606572785"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["Making use of sequence"],"metadata":{"id":"5CkozeWy1Z8O"}},{"cell_type":"code","source":["# univariate data preparation\n","from numpy import array\n","\n","# split a univariate sequence into samples\n","def split_sequence(sequence, n_steps):\n","\tX, y = list(), list()\n","\tfor i in range(len(sequence)):\n","\t\t# find the end of this pattern\n","\t\tend_ix = i + n_steps\n","\t\t# check if we are beyond the sequence\n","\t\tif end_ix > len(sequence)-1:\n","\t\t\tbreak\n","\t\t# gather input and output parts of the pattern\n","\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n","\t\tX.append(seq_x)\n","\t\ty.append(seq_y)\n","\treturn array(X), array(y)\n","\n","# define input sequence\n","raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n","# choose a number of time steps\n","n_steps = 3\n","# split into samples\n","X, y = split_sequence(raw_seq, n_steps)\n","# summarize the data\n","for i in range(len(X)):\n","\tprint(X[i], y[i])"],"metadata":{"id":"ty_kcY341b0d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680820815137,"user_tz":-330,"elapsed":3,"user":{"displayName":"Ayush Agarwal .","userId":"08062159735606572785"}},"outputId":"cefac34a-1e56-4221-881d-276b6075952a"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[10 20 30] 40\n","[20 30 40] 50\n","[30 40 50] 60\n","[40 50 60] 70\n","[50 60 70] 80\n","[60 70 80] 90\n"]}]},{"cell_type":"markdown","source":["Select Subset of Columns"],"metadata":{"id":"Tev2L_OukGV6"}},{"cell_type":"code","source":["import pandas as pd\n","\n","data = pd.read_csv (r'C:\\Users\\Ron\\Desktop\\Clients.csv')   \n","df = pd.DataFrame(data, columns= ['Person Name','Country'])\n","print (df)"],"metadata":{"id":"_ryvIpOLkDZJ","colab":{"base_uri":"https://localhost:8080/","height":398},"executionInfo":{"status":"error","timestamp":1680820818256,"user_tz":-330,"elapsed":6,"user":{"displayName":"Ayush Agarwal .","userId":"08062159735606572785"}},"outputId":"afddd4c0-317d-421f-83d6-417e19698165"},"execution_count":11,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-cd41ae4a8691>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mr'C:\\Users\\Ron\\Desktop\\Clients.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Person Name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Country'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Ron\\\\Desktop\\\\Clients.csv'"]}]},{"cell_type":"markdown","source":["Encoding Time-Series into Images for Forecasting using Convolutional Neural Networks\n"],"metadata":{"id":"YDoO9NRTy6IF"}},{"cell_type":"code","source":[" "],"metadata":{"id":"wEMcHhxvzGZx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Gramian Angular Fields (GAF)** are images representing a timeseries in the Polar Ordinate system (i.e. each point on the plane is determined by a distance from a reference point and an angle from a reference direction). Thus each GAF represents a temporal correlation between each time point.\n","\n"],"metadata":{"id":"M_TTl_i0zKP8"}},{"cell_type":"code","source":["import pandas as pd\n","import os\n","from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n","PATH = os.path.join(os.path.dirname(__file__), 'TimeSeries')\n","\n","def data_to_image_preprocess():\n","    \"\"\"\n","    :return: None\n","    \"\"\"\n","    ive_data = 'IVE_tickbidask.txt'\n","    col_name = ['Date', 'Time', 'Open', 'High', 'Low', 'Volume']\n","    df = pd.read_csv(os.path.join(PATH, ive_data), names=col_name, header=None)\n","    # Drop unnecessary data\n","    df = df.drop(['High', 'Low', 'Volume'], axis=1)\n","    df['DateTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'], infer_datetime_format=True)\n","    df = df.groupby(pd.Grouper(key='DateTime', freq='1h')).mean().reset_index()\n","    df['Open'] = df['Open'].replace(to_replace=0, method='ffill')\n","    # Remove non trading days and times\n","    clean_df = clean_non_trading_times(df)\n","    # Send to slicing\n","    set_gaf_data(clean_df)"],"metadata":{"id":"VbZpBMzAHkre","colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"status":"error","timestamp":1680820168292,"user_tz":-330,"elapsed":779,"user":{"displayName":"Ayush Agarwal .","userId":"08062159735606572785"}},"outputId":"ab6996af-e5d0-4a55-fd9c-42cbe571969f"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-e171df954837>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholiday\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUSFederalHolidayCalendar\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcalendar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TimeSeries'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata_to_image_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"]}]},{"cell_type":"code","source":["def clean_non_trading_times(df):\n","    \"\"\"\n","    :param df: Data with weekends and holidays\n","    :return trading_data:\n","    \"\"\"\n","    # Weekends go out\n","    df = df[df['DateTime'].dt.weekday < 5].reset_index(drop=True)\n","    df = df.set_index('DateTime')\n","    # Remove non trading hours\n","    df = df.between_time('9:00','16:00')\n","    df.reset_index(inplace=True)\n","    # Holiday days we want to delete from data\n","    holidays = calendar().holidays(start='2000-01-01', end='2020-12-31')\n","    m = df['DateTime'].isin(holidays)\n","    clean_df = df[~m].copy()\n","    trading_data = clean_df.fillna(method='ffill')\n","    return trading_data"],"metadata":{"id":"QpaZCMtfHxg7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def set_gaf_data(df):\n","    \"\"\"\n","    :param df: DataFrame data\n","    :return: None\n","    \"\"\"\n","    dates = df['DateTime'].dt.date\n","    dates = dates.drop_duplicates()\n","    list_dates = dates.apply(str).tolist()\n","    index = 20 # rows of data used on each GAF\n","    # Container to store data for the creation of GAF\n","    decision_map = {key: [] for key in ['LONG', 'SHORT']}\n","    while True:\n","        if index >= len(list_dates) - 1:\n","            break\n","        # Select appropriate timeframe\n","        data_slice = df.loc[(df['DateTime'] > list_dates[index - 20]) & (df['DateTime'] < list_dates[index])]\n","        gafs = []\n","        # Group data_slice by time frequency\n","        for freq in ['1h', '2h', '4h', '1d']:\n","            group_dt = data_slice.groupby(pd.Grouper(key='DateTime', freq=freq)).mean().reset_index()\n","            group_dt = group_dt.dropna()\n","            gafs.append(group_dt['Close'].tail(20))\n","        # Decide what trading position we should take on that day\n","        future_value = df[df['DateTime'].dt.date.astype(str) == list_dates[index]]['Close'].iloc[-1]\n","        current_value = data_slice['Close'].iloc[-1]\n","        decision = trading_action(future_close=future_value, current_close=current_value)\n","        decision_map[decision].append([list_dates[index - 1], gafs])\n","        index += 1"],"metadata":{"id":"-mTOgLsxH1A0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def trading_action(data, index):\n","    \"\"\"\n","    :param data: DataFrame \n","    :param index: Date Index for slicing\n","    :return: Folder destination as String\n","    \"\"\"\n","    future_open = data[data['DateTime'].dt.date.astype(str) == index]['Open'].iloc[0]\n","    future_close = data[data['DateTime'].dt.date.astype(str) == index]['Open'].iloc[-1]\n","    if future_open < future_close:\n","        decision = 'LONG'\n","    else:\n","        decision = 'SHORT'\n","    return decision"],"metadata":{"id":"vLAAkfrgH3Cq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_gaf(ts):\n","    \"\"\"\n","    :param ts:\n","    :return:\n","    \"\"\"\n","    data = dict()\n","    gadf = GramianAngularField(method='difference', image_size=ts.shape[0])\n","    data['gadf'] = gadf.fit_transform(pd.DataFrame(ts).T)[0] # ts.T)\n","    return data"],"metadata":{"id":"j_yjtshSH5k5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**THE MODEL**"],"metadata":{"id":"byyP1zAyH-6r"}},{"cell_type":"code","source":["cnn_networks = 3\n","model = []\n","for j in range(cnn_networks):\n","    model.append(\n","        tf.keras.models.Sequential([\n","            #  First Convolution\n","            Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(255, 255, 3)),\n","            BatchNormalization(),\n","            Conv2D(32, kernel_size=(3, 3), activation='relu'),\n","            BatchNormalization(),\n","            Conv2D(32, kernel_size=(3, 3), strides=2, padding='same', activation='relu'),\n","            BatchNormalization(),\n","            Dropout(0.4),\n","            # Second Convolution\n","            Conv2D(64, kernel_size=(3, 3), activation='relu'),\n","            BatchNormalization(),\n","            Conv2D(64, kernel_size=(3, 3), activation='relu'),\n","            BatchNormalization(),\n","            Conv2D(64, kernel_size=(3, 3), strides=2, padding='same', activation='relu'),\n","            BatchNormalization(),\n","            Dropout(0.4),\n","            # Third Convolution\n","            Conv2D(128, kernel_size=4, activation='relu'),\n","            BatchNormalization(),\n","            Flatten(),\n","            Dropout(0.4),\n","            # Output layer\n","            Dense(1, activation='sigmoid')]\n","        ))\n","    # Compile each model\n","    model[j].compile(optimizer=Adam(lr=LR), loss='binary_crossentropy', metrics=['acc'])"],"metadata":{"id":"lSPC2MwxIBT6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# All images will be rescaled by 1./255\n","train_validate_datagen = ImageDataGenerator(rescale=1/255, validation_split=SPLIT)  # set validation split\n","test_datagen = ImageDataGenerator(rescale=1/255)\n","data_chunks = ensemble_data(cnn_networks, IMAGES_PATH)\n","for j in range(cnn_networks):\n","    print('Net : {}'.format(j+1))\n","    df_train = data_chunks[j].iloc[:-60]\n","    df_test = data_chunks[j].iloc[-60:]\n","    train_generator = train_validate_datagen.flow_from_dataframe(\n","        dataframe=df_train,\n","        directory=IMAGES_PATH,\n","        target_size=(255, 255),\n","        x_col='Images',\n","        y_col='Labels',\n","        batch_size=32,\n","        class_mode='binary',\n","        subset='training')\n","\n","    validation_generator = train_validate_datagen.flow_from_dataframe(\n","        dataframe=df_train,\n","        directory=IMAGES_PATH,\n","        target_size=(255, 255),\n","        x_col='Images',\n","        y_col='Labels',\n","        batch_size=32,\n","        class_mode='binary',\n","        subset='validation')\n","\n","    test_generator = test_datagen.flow_from_dataframe(\n","        dataframe=df_test,\n","        x_col='Images',\n","        y_col='Labels',\n","        directory=IMAGES_PATH,\n","        target_size=(255, 255),\n","        class_mode='binary')"],"metadata":{"id":"ricVc_WaIQVC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["steps_per_epoch = train_generator.n // train_generator.batch_size\n","        validation_steps = validation_generator.n // validation_generator.batch_size\n","        learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=0, factor=0.5, min_lr=0.00001)\n","        history = model[j].fit_generator(train_generator,\n","                                         epochs=EPOCHS,\n","                                         steps_per_epoch=steps_per_epoch,\n","                                         validation_data=validation_generator,\n","                                         callbacks=[learning_rate_reduction],\n","                                         verbose=0)\n","        print('CNN Model {0:d}: '\n","              'Epochs={1:d}, '\n","              'Training Accuracy={2:.5f}, '\n","              'Validation Accuracy={3:.5f}'.format(j + 1,\n","                                                   EPOCHS,\n","                                                   max(history.history['acc']),\n","                                                   max(history.history['val_acc'])))\n","\n","        scores = model[j].evaluate_generator(test_generator, steps=5)\n","        print(\"{0}s: {1:.2f}%\".format(model[j].metrics_names[1], scores[1]*100))\n","        string_list = []\n","        model[j].summary(print_fn=lambda x: string_list.append(x))\n","        summary = \"\\n\".join(string_list)\n","        logging = ['{0}: {1}'.format(key, val[-1]) for key, val in history.history.items()]\n","        log = 'Results:\\n' + '\\n'.join(logging)\n","        model[j].save(os.path.join(REPO, 'computer_vision_model_{0}_of_{1}_{2}.h5'.format(TIMESTAMP, j, cnn_networks)))\n","        f = open(os.path.join(REPO, 'computer_vision_summary_{0}_of_{1}_{2}.h5'.format(TIMESTAMP, j, cnn_networks)), 'w')\n","        f.write(\"EPOCHS: {0}\\nSteps per epoch: {1}\\nValidation steps: {2}\\nVal Split:{3}\\nLearning RT:{5}\\n\\n\\n{4}\"\n","                \"\\n\\n=========TRAINING LOG========\\n{6}\".format(EPOCHS, steps_per_epoch, validation_steps,  SPLIT, summary,\n","                                                                LR, log))\n","        f.close()steps_per_epoch = train_generator.n // train_generator.batch_size\n","        validation_steps = validation_generator.n // validation_generator.batch_size\n","        learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=0, factor=0.5, min_lr=0.00001)\n","        history = model[j].fit_generator(train_generator,\n","                                         epochs=EPOCHS,\n","                                         steps_per_epoch=steps_per_epoch,\n","                                         validation_data=validation_generator,\n","                                         callbacks=[learning_rate_reduction],\n","                                         verbose=0)\n","        print('CNN Model {0:d}: '\n","              'Epochs={1:d}, '\n","              'Training Accuracy={2:.5f}, '\n","              'Validation Accuracy={3:.5f}'.format(j + 1,\n","                                                   EPOCHS,\n","                                                   max(history.history['acc']),\n","                                                   max(history.history['val_acc'])))\n","\n","        scores = model[j].evaluate_generator(test_generator, steps=5)\n","        print(\"{0}s: {1:.2f}%\".format(model[j].metrics_names[1], scores[1]*100))\n","        string_list = []\n","        model[j].summary(print_fn=lambda x: string_list.append(x))\n","        summary = \"\\n\".join(string_list)\n","        logging = ['{0}: {1}'.format(key, val[-1]) for key, val in history.history.items()]\n","        log = 'Results:\\n' + '\\n'.join(logging)\n","        model[j].save(os.path.join(REPO, 'computer_vision_model_{0}_of_{1}_{2}.h5'.format(TIMESTAMP, j, cnn_networks)))\n","        f = open(os.path.join(REPO, 'computer_vision_summary_{0}_of_{1}_{2}.h5'.format(TIMESTAMP, j, cnn_networks)), 'w')\n","        f.write(\"EPOCHS: {0}\\nSteps per epoch: {1}\\nValidation steps: {2}\\nVal Split:{3}\\nLearning RT:{5}\\n\\n\\n{4}\"\n","                \"\\n\\n=========TRAINING LOG========\\n{6}\".format(EPOCHS, steps_per_epoch, validation_steps,  SPLIT, summary,\n","                                                                LR, log))\n","        f.close()"],"metadata":{"id":"sLJoDhCyIVVG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Recurrence Plot**\n"],"metadata":{"id":"knys7f8i2x9V"}},{"cell_type":"code","source":["from pyts.datasets import load_gunpoint\n","from pyts.image import RecurrencePlot\n","X, _, _, _ = load_gunpoint(return_X_y=True)\n","transformer = RecurrencePlot()\n","X_new = transformer.transform(X)\n","X_new.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"id":"gG94_bYd26ZN","executionInfo":{"status":"error","timestamp":1663698211026,"user_tz":-330,"elapsed":8,"user":{"displayName":"Ayush Agarwal .","userId":"08062159735606572785"}},"outputId":"fdbca629-4772-4b65-8566-397769dea562"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-0a89724bf5c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_gunpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRecurrencePlot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_gunpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_X_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtransformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRecurrencePlot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyts'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"markdown","source":["Images can be flattened by setting the flatten parameter to True, so that classification can be directly performed:"],"metadata":{"id":"y_hVgTSa3EAM"}},{"cell_type":"code","source":["from pyts.image import RecurrencePlot\n","from pyts.datasets import load_gunpoint\n","from sklearn.pipeline import make_pipeline\n","from sklearn.linear_model import LogisticRegression\n","X_train, X_test, y_train, y_test = load_gunpoint(return_X_y=True)\n","recurrence = RecurrencePlot(dimension=15, time_delay=3, flatten=True)\n","logistic = LogisticRegression(solver='liblinear')\n","clf = make_pipeline(recurrence, logistic)\n","clf.fit(X_train, y_train)\n","\n","clf.score(X_test, y_test)\n"],"metadata":{"id":"A0casVzb3Ddp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Gramian Angular Field\n","{making use of pyts)"],"metadata":{"id":"j4Dk9CcFGy12"}},{"cell_type":"code","source":["from pyts.datasets import load_gunpoint\n","from pyts.image import GramianAngularField\n","X, _, _, _ = load_gunpoint(return_X_y=True)\n","transformer = GramianAngularField()\n","X_new = transformer.transform(X)\n","X_new.shape"],"metadata":{"id":"Udd3eUiGG1MJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Images can be flattened by setting the flatten parameter to True, so that classification can be directly performed:"],"metadata":{"id":"Fa90zo6YG7K8"}},{"cell_type":"code","source":["from pyts.image import GramianAngularField\n","from pyts.datasets import load_gunpoint\n","from sklearn.pipeline import make_pipeline\n","from sklearn.linear_model import LogisticRegression\n","X_train, X_test, y_train, y_test = load_gunpoint(return_X_y=True)\n","gaf = GramianAngularField(flatten=True)\n","logistic = LogisticRegression(solver='liblinear')\n","clf = make_pipeline(gaf, logistic)\n","clf.fit(X_train, y_train)\n","\n","clf.score(X_test, y_test)"],"metadata":{"id":"Riz8WdbQG4IJ"},"execution_count":null,"outputs":[]}]}